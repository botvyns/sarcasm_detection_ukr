{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Use gpt-4-turbo for sarcastic texts calssification in Ukrainian.**\n",
        "\n",
        "### Classification is performed on test portion of data to compare with other used models"
      ],
      "metadata": {
        "id": "mTpw8HFXSmo0"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "04WuFWCaOMAZ"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "!pip install openai"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from openai import OpenAI\n",
        "from google.colab import userdata\n",
        "import json\n",
        "import pandas as pd"
      ],
      "metadata": {
        "id": "IgbXMx1cPoCD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test = pd.read_csv(\"test_ready_for_models.csv\")"
      ],
      "metadata": {
        "id": "KDxjMffW2f0k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test['text_mod'] = test['text_mod'].replace('\\n', '').str.strip()"
      ],
      "metadata": {
        "id": "Svk7bG-F3i_z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "system_setting = \"\"\"Ти багато спілкуєшся онлайн. Тобі відомо, що сарказм – таке висловлювання, буквальне значення якого відрізняється від того, яке мовець насправді має на увазі.\n",
        "Саркастичні тексти можуть містити такі ознаки: гіпербола, пунктуаційні знаки, прагматичні ознаки (емотикони, емоджі, великі літери), невідповідність, пародіювання російської вимови.\n",
        "\"\"\"\n",
        "\n",
        "user_input = \"\"\"Класифікуй вказані тексти у зворотних лапках на один з класів: `сарказм`: 1,  `не сарказм`: 0. Поверни правильний RFC8259 JSON без жодних відхилень у такому форматі:\n",
        "{{\"класи\": list[клас для кожного тексту]}}\n",
        "1. `{0}`\n",
        "2. `{1}`\n",
        "3. `{2}`\n",
        "4. `{3}`\n",
        "5. `{4}`\n",
        "6. `{5}`\n",
        "7. `{6}`\n",
        "8. `{7}`\n",
        "9. `{8}`\n",
        "10.`{9}`\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "NsgPuIYyc0o4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "COMPLETIONS_MODEL =\"gpt-3.5-turbo\"\n",
        "\n",
        "client = OpenAI(api_key=userdata.get('OPENAI_API_KEY'))"
      ],
      "metadata": {
        "id": "ylbr1E4_Pd5J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "\n",
        "def parse_string(llm_output):\n",
        "    \"\"\"\n",
        "    Parses the output of a language model and extracts labels.\n",
        "\n",
        "    Parameters:\n",
        "    llm_output (str): The output of a language model in JSON format.\n",
        "\n",
        "    Returns:\n",
        "    list: A list of labels extracted from the JSON output.\n",
        "    \"\"\"\n",
        "    json_llm_output = json.loads(llm_output)\n",
        "    labels = json_llm_output[\"класи\"]\n",
        "    return labels\n"
      ],
      "metadata": {
        "id": "G0XQqNUNSP4H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def request_completion_openai(system_setting, user_input):\n",
        "    \"\"\"\n",
        "    Requests completion from the OpenAI Chat API using the provided system setting and user input.\n",
        "\n",
        "    Parameters:\n",
        "    system_setting (str): The system setting to be included in the chat completion request.\n",
        "    user_input (str): The user input to be included in the chat completion request.\n",
        "\n",
        "    Returns:\n",
        "    dict: A dictionary containing the completion response from the OpenAI Chat API.\n",
        "    \"\"\"\n",
        "    completion_response = client.chat.completions.create(\n",
        "        messages=[\n",
        "            {\"role\": \"system\", \"content\": system_setting},\n",
        "            {\"role\": \"user\", \"content\": user_input}\n",
        "        ],\n",
        "        model=COMPLETIONS_MODEL\n",
        "    )\n",
        "    return completion_response\n",
        "\n",
        "def classify_texts_openai(messages, user_input, system_setting):\n",
        "    \"\"\"\n",
        "    Classifies texts using the OpenAI Chat API based on provided messages, user input, and system setting.\n",
        "\n",
        "    Parameters:\n",
        "    messages (list): A list of messages to be used in formatting the user input.\n",
        "    user_input (str): The user input string with formatting placeholders.\n",
        "    system_setting (str): The system setting to be included in the chat completion request.\n",
        "\n",
        "    Returns:\n",
        "    str: The classification result obtained from the OpenAI Chat API.\n",
        "    \"\"\"\n",
        "    user_input_combined = user_input.format(*messages)\n",
        "    classification = request_completion_openai(system_setting, user_input_combined).choices[0].message.content.replace('\\n', '')\n",
        "    return classification"
      ],
      "metadata": {
        "id": "cfKUjjpTQHBJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Perform annotation\n",
        "texts = test['text_mod'].tolist()[:2150]\n",
        "num_texts = len(texts)\n",
        "\n",
        "batch_size = 10\n",
        "\n",
        "openai_pred = []\n",
        "\n",
        "for i in range(0, num_texts, batch_size):\n",
        "     messages = texts[i:i+batch_size]\n",
        "     open_ai_ouput = classify_texts_openai(messages, user_input, system_setting)\n",
        "     openai_pred.append(open_ai_ouput)"
      ],
      "metadata": {
        "id": "hfUDsBbJ42Vv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Perform annotation on left texts\n",
        "user_input = \"\"\"Класифікуй вказані тексти у зворотних лапках на один з класів: `сарказм`: 1,  `не сарказм`: 0. Поверни правильний RFC8259 JSON без жодних відхилень у такому форматі:\n",
        "{{\"класи\": list[клас для кожного тексту]}}\n",
        "1. `{0}`\n",
        "2. `{1}`\n",
        "\"\"\"\n",
        "\n",
        "texts = test['text_mod'].tolist()[2150:]\n",
        "num_texts = len(texts)\n",
        "\n",
        "batch_size = 10\n",
        "\n",
        "openai_pred_2 = []\n",
        "\n",
        "messages = texts\n",
        "open_ai_ouput = classify_texts_openai(messages, user_input, system_setting)\n",
        "openai_pred_2.append(open_ai_ouput)"
      ],
      "metadata": {
        "id": "1-qHnYKH8zb7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "openai_pred.extend(openai_pred_2) #concat all predictions"
      ],
      "metadata": {
        "id": "vE3c4FHZCJRu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Clear LLM output\n",
        "cleaned_openai_pred = []\n",
        "\n",
        "for item in openai_pred:\n",
        "  try:\n",
        "    cleaned_openai_pred.extend(parse_string(item))\n",
        "  except:\n",
        "    temp = item.replace(\"```json\", \"\").replace(\"```\", \"\")\n",
        "    cleaned_openai_pred.extend(parse_string(temp))"
      ],
      "metadata": {
        "id": "9cFWD8jqA-45"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "\n",
        "def calculate_metrics(true_labels, predicted_labels):\n",
        "    \"\"\"\n",
        "    Calculates evaluation metrics such as accuracy, precision, recall, and F1-score.\n",
        "\n",
        "    Parameters:\n",
        "    true_labels (array-like): The true labels.\n",
        "    predicted_labels (array-like): The predicted labels.\n",
        "\n",
        "    Returns:\n",
        "    tuple: A tuple containing the calculated metrics in the order: (accuracy, precision, recall, f1).\n",
        "    \"\"\"\n",
        "    # Calculate metrics\n",
        "    accuracy = accuracy_score(true_labels, predicted_labels)\n",
        "    precision = precision_score(true_labels, predicted_labels)\n",
        "    recall = recall_score(true_labels, predicted_labels)\n",
        "    f1 = f1_score(true_labels, predicted_labels)\n",
        "\n",
        "    return accuracy, precision, recall, f1\n"
      ],
      "metadata": {
        "id": "0gubw9NgSL5a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "accuracy, precision, recall, f1 = calculate_metrics(test['is_sarcastic'].tolist(), cleaned_openai_pred)\n",
        "\n",
        "print(\"Accuracy:\", accuracy)\n",
        "print(\"Precision:\", precision)\n",
        "print(\"Recall:\", recall)\n",
        "print(\"F1-score:\", f1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EeZ3vAkVcRHR",
        "outputId": "cc489e69-c020-4980-f929-daad2bc154af"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.5762081784386617\n",
            "Precision: 0.5635658914728682\n",
            "Recall: 0.6756505576208178\n",
            "F1-score: 0.6145393068469991\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "dj9SxFBRDl47"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}